{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13aeb7ef042c990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:10:39.319759Z",
     "start_time": "2024-05-30T16:10:38.742007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from needle_dictionary import Needles, Cities\n",
    "\n",
    "def haystack_builder(df, k, tokenizer, buffer_size=100):\n",
    "    # Initialize variables\n",
    "    concatenated_body = \"\"\n",
    "    total_tokens = 0\n",
    "    articles_added = 0\n",
    "    first_article_added = False  # Flag to indicate whether the first article has been added\n",
    "\n",
    "    # Iterate through each article in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Token size of the article body\n",
    "        try:\n",
    "            article_length = len(tokenizer.tokenize(row[\"body\"]))\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                article_length = len(tokenizer.encode(row[\"body\"]))\n",
    "            except AttributeError:\n",
    "                article_length = tokenizer(row[\"body\"]).total_tokens\n",
    "                time.sleep(1)\n",
    "\n",
    "        # Check if adding the current article would exceed the token limit\n",
    "        if not first_article_added and total_tokens + article_length > k * 1000 - buffer_size:\n",
    "            # Skip the first article if it exceeds the token limit\n",
    "            continue\n",
    "\n",
    "        # Set the flag to indicate that the first article has been added\n",
    "        first_article_added = True\n",
    "\n",
    "        # Check if adding the current article would exceed the token limit\n",
    "        if total_tokens + article_length <= k * 1000 - buffer_size:\n",
    "            # Concatenate the body of the article\n",
    "            concatenated_body += row[\"body\"] + \" \"\n",
    "\n",
    "            # Update total tokens and count of articles added\n",
    "            total_tokens += article_length\n",
    "        else:\n",
    "            # Sentence tokenize the last article\n",
    "            sentences = nltk.sent_tokenize(row[\"body\"])\n",
    "            for sentence in sentences:\n",
    "                # Token length of the sentence\n",
    "                try:\n",
    "                    sentence_length = len(tokenizer.tokenize(sentence))\n",
    "                except AttributeError:\n",
    "                    try:\n",
    "                        sentence_length = len(tokenizer.encode(sentence))\n",
    "                    except AttributeError:\n",
    "                        sentence_length = tokenizer(sentence).total_tokens\n",
    "                        time.sleep(1)\n",
    "                # Check if adding the sentence would exceed the token limit\n",
    "                if total_tokens + sentence_length <= k * 1000 - buffer_size:\n",
    "                    # Concatenate the sentence\n",
    "                    concatenated_body += sentence + \" \"\n",
    "                    # Update total tokens\n",
    "                    total_tokens += sentence_length\n",
    "                else:\n",
    "                    # Break the loop if adding the sentence would exceed the token limit\n",
    "                    break\n",
    "            # Break the loop after processing the last article\n",
    "            break\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    return concatenated_body\n",
    "\n",
    "\n",
    "def split_into_parts(text, tokenizer, percentage):\n",
    "    if percentage == 0:\n",
    "        return [\"\", text]\n",
    "    elif percentage == 100:\n",
    "        return [text, \"\"]\n",
    "    else:\n",
    "        # Splitting text into sentences\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "        # Calculating number of tokens\n",
    "        try:\n",
    "            total_tokens = len(tokenizer.tokenize(text))\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                total_tokens = len(tokenizer.encode(text))\n",
    "            except AttributeError:\n",
    "                total_tokens = tokenizer(text).total_tokens\n",
    "                time.sleep(1)\n",
    "\n",
    "        # Calculating the desired number of tokens for first part\n",
    "        tokens_first_part = int(total_tokens * percentage/100)\n",
    "\n",
    "        approximated_i = [1, 4]\n",
    "\n",
    "        while approximated_i[-1] != approximated_i[-2]:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            approximated_i.append(int(tokens_first_part / current_length * approximated_i[-1]))\n",
    "            if approximated_i[-1] in approximated_i[:-2]:\n",
    "                approximated_i.append(min(approximated_i[-3:-1]))\n",
    "                break\n",
    "        while True:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            if current_length < tokens_first_part:\n",
    "                approximated_i.append(approximated_i[-1] + 1)\n",
    "            else:\n",
    "                break\n",
    "        parts = [' '.join(sentences[:approximated_i[-1]]), ' '.join(sentences[approximated_i[-1]:])]\n",
    "        print(k, approximated_i)\n",
    "        return parts\n",
    "\n",
    "\n",
    "def generate_random_number(n_digits):\n",
    "    lower_bound = 10**(n_digits-1)\n",
    "    upper_bound = (10**n_digits)-1\n",
    "    return random.randint(lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "def needles_builder(language, n_digits=7):\n",
    "    # select a random city\n",
    "    city = random.choice(Cities)[language]\n",
    "    # select a random number with n_digits digits\n",
    "    rnd_number = generate_random_number(n_digits)\n",
    "    # create the needle\n",
    "    needle = Needles[language].format(city=city, number=rnd_number)\n",
    "    return needle, city, rnd_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5498271c088ca9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:11:18.997780Z",
     "start_time": "2024-05-30T16:11:10.053163Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1839\n",
      "2 [1, 4, 5, 5, 6, 7, 8]\n",
      "2 [1, 4, 10, 15, 15, 16, 17, 18]\n",
      "2 [1, 4, 15, 23, 22, 22, 23]\n",
      "Total tokens: 7887\n",
      "8 [1, 4, 21, 31, 34, 34, 35, 36]\n",
      "8 [1, 4, 43, 73, 72, 72, 73]\n",
      "8 [1, 4, 65, 99, 124, 123, 122, 123, 122, 123]\n",
      "Total tokens: 15891\n",
      "16 [1, 4, 43, 74, 73, 73, 74]\n",
      "16 [1, 4, 87, 156, 161, 162, 163, 164, 164, 165, 166, 167]\n",
      "16 [1, 4, 130, 239, 247, 252, 254, 255, 255, 256, 257]\n",
      "Total tokens: 31889\n",
      "32 [1, 4, 87, 156, 161, 163, 164, 165, 166, 166, 167, 168]\n",
      "32 [1, 4, 175, 337, 367, 360, 358, 362, 357, 362, 357, 358, 359, 360]\n",
      "32 [1, 4, 262, 522, 509, 509, 510]\n",
      "Total tokens: 63852\n",
      "64 [1, 4, 175, 338, 366, 360, 359, 363, 358, 363, 358, 359, 360]\n",
      "64 [1, 4, 350, 720, 733, 737, 738, 739, 739, 740]\n",
      "64 [1, 4, 526, 1022, 1062, 1061, 1060, 1060, 1061]\n",
      "Total tokens: 1871\n",
      "2 [1, 4, 1, 1, 2]\n",
      "2 [1, 4, 2, 3, 2, 2, 3]\n",
      "2 [1, 4, 4, 5, 6]\n",
      "Total tokens: 7840\n",
      "8 [1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 17, 18]\n",
      "8 [1, 4, 11, 25, 26, 26, 27, 28]\n",
      "8 [1, 4, 17, 52, 42, 45, 42, 42, 43, 44, 45]\n",
      "Total tokens: 15881\n",
      "16 [1, 4, 12, 27, 27, 28, 29]\n",
      "16 [1, 4, 24, 51, 57, 61, 63, 64, 65, 66, 66, 67, 68]\n",
      "16 [1, 4, 36, 100, 121, 126, 126, 127]\n",
      "Total tokens: 31864\n",
      "32 [1, 4, 24, 51, 57, 61, 63, 65, 66, 67, 67, 68]\n",
      "32 [1, 4, 48, 119, 167, 157, 153, 152, 151, 151, 152]\n",
      "32 [1, 4, 72, 209, 221, 221, 222]\n",
      "Total tokens: 63811\n",
      "64 [1, 4, 48, 120, 168, 157, 153, 152, 151, 151, 152]\n",
      "64 [1, 4, 96, 317, 290, 289, 288, 287, 287, 288]\n",
      "64 [1, 4, 145, 451, 450, 450, 451]\n",
      "Total tokens: 1839\n",
      "2 [1, 4, 14, 12, 14, 12, 13]\n",
      "2 [1, 4, 29, 24, 25, 24, 24, 25]\n",
      "2 [1, 4, 44, 40, 40, 41]\n",
      "Total tokens: 7836\n",
      "8 [1, 4, 63, 40, 57, 40, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "8 [1, 4, 127, 63, 80, 68, 80, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n",
      "8 [1, 4, 190, 142, 106, 85, 103, 84, 102, 85, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
      "Total tokens: 15860\n",
      "16 [1, 4, 128, 57, 81, 70, 81, 70, 71, 72, 73, 74, 75, 76]\n",
      "16 [1, 4, 257, 128, 115, 116, 116, 117]\n",
      "16 [1, 4, 386, 288, 215, 172, 173, 173, 174]\n",
      "Total tokens: 31738\n",
      "32 [1, 4, 257, 117, 112, 115, 116, 116, 117]\n",
      "32 [1, 4, 515, 256, 234, 231, 233, 232, 232, 233]\n",
      "32 [1, 4, 773, 578, 432, 371, 380, 378, 378, 379]\n",
      "Total tokens: 63882\n",
      "64 [1, 4, 519, 251, 232, 234, 233, 233, 234]\n",
      "64 [1, 4, 1038, 517, 501, 503, 501, 501, 502, 503]\n",
      "64 [1, 4, 1557, 1163, 869, 742, 746, 747, 747, 748, 749]\n",
      "Total tokens: 1832\n",
      "2 [1, 4, 2, 2, 3]\n",
      "2 [1, 4, 4]\n",
      "2 [1, 4, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14]\n",
      "Total tokens: 7892\n",
      "8 [1, 4, 49, 47, 46, 47, 46, 47]\n",
      "8 [1, 4, 99, 72, 86, 72, 72, 73, 74, 75]\n",
      "8 [1, 4, 148, 110, 110, 111]\n",
      "Total tokens: 15884\n",
      "16 [1, 4, 99, 73, 78, 70, 89, 72, 86, 73, 72, 73, 74, 75]\n",
      "16 [1, 4, 199, 129, 154, 143, 145, 146, 146, 147]\n",
      "16 [1, 4, 299, 224, 206, 198, 193, 193, 194]\n",
      "Total tokens: 31892\n",
      "32 [1, 4, 200, 130, 155, 138, 148, 146, 147, 146, 146, 147]\n",
      "32 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284]\n",
      "32 [1, 4, 601, 449, 422, 422, 423, 424]\n",
      "Total tokens: 63799\n",
      "64 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284]\n",
      "64 [1, 4, 802, 589, 582, 578, 577, 577, 578]\n",
      "64 [1, 4, 1203, 900, 863, 874, 873, 873, 874]\n",
      "Total tokens: 1862\n",
      "2 [1, 4, 7, 8, 8, 9]\n",
      "2 [1, 4, 14, 15, 14, 14, 15]\n",
      "2 [1, 4, 22, 23, 24, 25, 26, 26, 27]\n",
      "Total tokens: 7857\n",
      "8 [1, 4, 34, 24, 21, 22, 20, 22, 20, 21, 22]\n",
      "8 [1, 4, 69, 54, 56, 57, 58, 58, 59]\n",
      "8 [1, 4, 104, 78, 76, 77, 77, 78]\n",
      "Total tokens: 15690\n",
      "16 [1, 4, 69, 54, 56, 57, 57, 58, 59]\n",
      "16 [1, 4, 139, 99, 101, 100, 100, 101]\n",
      "16 [1, 4, 209, 156, 149, 148, 148, 149]\n",
      "Total tokens: 31889\n",
      "32 [1, 4, 141, 101, 102, 102, 103]\n",
      "32 [1, 4, 283, 235, 208, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186]\n",
      "32 [1, 4, 424, 424, 425, 426]\n",
      "Total tokens: 63756\n",
      "64 [1, 4, 283, 236, 209, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186]\n",
      "64 [1, 4, 566, 589, 579, 580, 580, 581]\n",
      "64 [1, 4, 850, 784, 790, 783, 790, 783, 784, 785, 786, 787]\n",
      "Total tokens: 1894\n",
      "2 [1, 4, 4, 5]\n",
      "2 [1, 4, 9, 7, 6, 7, 6, 7]\n",
      "2 [1, 4, 13, 13, 14]\n",
      "Total tokens: 7879\n",
      "8 [1, 4, 19, 23, 25, 27, 28, 29, 29, 30, 31]\n",
      "8 [1, 4, 38, 61, 70, 75, 67, 74, 70, 67, 68, 69, 70, 71, 72]\n",
      "8 [1, 4, 58, 103, 79, 96, 82, 95, 83, 92, 85, 91, 86, 91, 86, 87, 88]\n",
      "Total tokens: 15884\n",
      "16 [1, 4, 39, 62, 71, 72, 70, 75, 68, 75, 68, 69, 70, 71, 72]\n",
      "16 [1, 4, 78, 134, 98, 111, 104, 105, 106, 106, 107]\n",
      "16 [1, 4, 117, 158, 147, 140, 144, 143, 144, 143, 144]\n",
      "Total tokens: 31715\n",
      "32 [1, 4, 77, 133, 97, 111, 104, 105, 106, 106, 107]\n",
      "32 [1, 4, 155, 193, 190, 190, 191, 192]\n",
      "32 [1, 4, 233, 281, 281, 282]\n",
      "Total tokens: 63871\n",
      "64 [1, 4, 157, 196, 192, 193, 192, 192, 193]\n",
      "64 [1, 4, 314, 377, 381, 378, 382, 378, 378, 379, 380]\n",
      "64 [1, 4, 471, 563, 590, 588, 589, 588, 588, 589]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "import google.generativeai as genai\n",
    "from config import api_key_google\n",
    "from huggingface_hub import login\n",
    "import time\n",
    "\n",
    "tokenizer_name = 'GPT4o'\n",
    "\n",
    "match tokenizer_name:\n",
    "    case \"GPT4\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    case \"GPT4o\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    case \"Claude\":\n",
    "        tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')\n",
    "    case \"Gemini\":\n",
    "        genai.configure(api_key=api_key_google)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\")\n",
    "        tokenizer = model.count_tokens\n",
    "    case \"YaRN\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Yarn-Llama-2-7b-128k\")\n",
    "    case \"Claude_downloaded\":\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/claude-v1-tokenization.json\")\n",
    "    case \"Llama\":\n",
    "        login(\"your_Huggingface_token\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    case _:\n",
    "        raise ValueError(\"Unknown tokenizer_name specified. Please provide a valid tokenizer_name.\")\n",
    "\n",
    "ks = [2, 8, 16, 32, 64]\n",
    "\n",
    "for language in ['English', 'Somali', 'Swahili', 'Indonesian', 'Azeri', 'Vietnamese']:\n",
    "    df_return = pd.DataFrame()\n",
    "    df = pd.read_csv(f\"datasets/Hastacks/mBBC_2024/bbc_{language}.csv\")\n",
    "    for k in ks:\n",
    "        haystack = haystack_builder(df, k, tokenizer)\n",
    "        \n",
    "        for percentage in [0, 25, 50, 75, 100]:\n",
    "            parts = split_into_parts(haystack, tokenizer, percentage)\n",
    "            needle, city, rnd_number = needles_builder(language)\n",
    "            data = {\n",
    "                \"context_length\": f\"{k}k\",\n",
    "                \"position\": percentage,\n",
    "                \"text\": parts[0] + ' ' + needle + ' ' + parts[1],\n",
    "                \"city\": city,\n",
    "                \"label\": rnd_number\n",
    "            }\n",
    "            df_return = pd.concat([df_return, pd.DataFrame([data])], ignore_index=True)\n",
    "            df_return.to_csv(f\"datasets/Haystack_Needles/Incongruous/1-Needle/mBBC_2024/{tokenizer_name}/{language}_needles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1dfe75457bd39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Two-needles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0d34bded3197e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:11:59.545796Z",
     "start_time": "2024-05-30T16:11:59.538193Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def split_into_3_parts(text, tokenizer, percentage):\n",
    "\n",
    "    # Splitting text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Calculating number of tokens\n",
    "    try:\n",
    "        total_tokens = len(tokenizer.tokenize(text))\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            total_tokens = len(tokenizer.encode(text))\n",
    "        except AttributeError:\n",
    "            total_tokens = tokenizer(text).total_tokens\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Calculating the desired number of tokens for first part\n",
    "    tokens_first_part = int(total_tokens * percentage/100)\n",
    "    tokens_second_part = int(total_tokens * (percentage + 25)/100)\n",
    "\n",
    "    approximated_i1 = [1, 4]\n",
    "    \n",
    "    if percentage == 0:\n",
    "        approximated_i1 = [0]\n",
    "    else:\n",
    "        while approximated_i1[-1] != approximated_i1[-2]:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i1[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            approximated_i1.append(int(tokens_first_part / current_length * approximated_i1[-1]))\n",
    "            if approximated_i1[-1] in approximated_i1[:-2]:\n",
    "                approximated_i1.append(min(approximated_i1[-3:-1]))\n",
    "                break\n",
    "        while True:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i1[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            if current_length < tokens_first_part:\n",
    "                approximated_i1.append(approximated_i1[-1] + 1)\n",
    "            else:\n",
    "                break\n",
    "    if (percentage + 25) != 100:\n",
    "        approximated_i2 = [approximated_i1[-1], approximated_i1[-1] + 1]\n",
    "        while approximated_i2[-1] != approximated_i2[-2]:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i2[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            approximated_i2.append(int(tokens_second_part / current_length * approximated_i2[-1]))\n",
    "            if approximated_i2[-1] in approximated_i2[:-2]:\n",
    "                approximated_i2.append(min(approximated_i2[-3:-1]))\n",
    "                break\n",
    "        while True:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i2[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            if current_length < tokens_second_part:\n",
    "                approximated_i2.append(approximated_i2[-1] + 1)\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        approximated_i2 = [len(sentences)]\n",
    "    \n",
    "    random_choice = random.randint(approximated_i1[-1], approximated_i2[-1])\n",
    "    \n",
    "    parts = [' '.join(sentences[:approximated_i1[-1]]), ' '.join(sentences[approximated_i1[-1]:random_choice]), ' '.join(sentences[random_choice:])]\n",
    "    print(k, approximated_i1, approximated_i2)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa188bedc110696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:12:42.994883Z",
     "start_time": "2024-05-30T16:12:29.031891Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1839\n",
      "2 [0] [0, 1, 5, 5, 6, 7, 8]\n",
      "2 [1, 4, 5, 5, 6, 7, 8] [8, 9, 15, 15, 16, 17, 18]\n",
      "2 [1, 4, 10, 15, 15, 16, 17, 18] [18, 19, 22, 22, 23]\n",
      "2 [1, 4, 15, 23, 22, 22, 23] [33]\n",
      "Total tokens: 7887\n",
      "8 [0] [0, 1, 22, 31, 34, 34, 35, 36]\n",
      "8 [1, 4, 21, 31, 34, 34, 35, 36] [36, 37, 72, 72, 73]\n",
      "8 [1, 4, 43, 73, 72, 72, 73] [73, 74, 109, 123, 122, 123, 122, 123]\n",
      "8 [1, 4, 65, 99, 124, 123, 122, 123, 122, 123] [166]\n",
      "Total tokens: 15891\n",
      "16 [0] [0, 1, 45, 70, 70, 71, 72, 73, 74]\n",
      "16 [1, 4, 43, 74, 73, 73, 74] [74, 75, 147, 159, 161, 162, 163, 164, 164, 165, 166, 167]\n",
      "16 [1, 4, 87, 156, 161, 162, 163, 164, 164, 165, 166, 167] [167, 168, 250, 253, 254, 255, 255, 256, 257]\n",
      "16 [1, 4, 130, 239, 247, 252, 254, 255, 255, 256, 257] [360]\n",
      "Total tokens: 31889\n",
      "32 [0] [0, 1, 91, 158, 162, 163, 164, 165, 166, 166, 167, 168]\n",
      "32 [1, 4, 87, 156, 161, 163, 164, 165, 166, 166, 167, 168] [168, 169, 333, 367, 360, 358, 362, 357, 362, 357, 358, 359, 360]\n",
      "32 [1, 4, 175, 337, 367, 360, 358, 362, 357, 362, 357, 358, 359, 360] [360, 361, 537, 513, 508, 511, 508, 508, 509, 510]\n",
      "32 [1, 4, 262, 522, 509, 509, 510] [744]\n",
      "Total tokens: 63852\n",
      "64 [0] [0, 1, 183, 342, 360, 359, 363, 358, 363, 358, 359, 360]\n",
      "64 [1, 4, 175, 338, 366, 360, 359, 363, 358, 363, 358, 359, 360] [360, 361, 717, 730, 740, 739, 739, 740]\n",
      "64 [1, 4, 350, 720, 733, 737, 738, 739, 739, 740] [740, 741, 1110, 1054, 1058, 1058, 1059, 1060, 1061]\n",
      "64 [1, 4, 526, 1022, 1062, 1061, 1060, 1060, 1061] [1398]\n",
      "Total tokens: 1871\n",
      "2 [0] [0, 1, 3, 1, 1, 2]\n",
      "2 [1, 4, 1, 1, 2] [2, 3, 2, 2, 3]\n",
      "2 [1, 4, 2, 3, 2, 2, 3] [3, 4, 4, 5, 6]\n",
      "2 [1, 4, 4, 5, 6] [16]\n",
      "Total tokens: 7840\n",
      "8 [0] [0, 1, 14, 15, 16, 16, 17, 18]\n",
      "8 [1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 17, 18] [18, 19, 36, 33, 30, 28, 27, 27, 28]\n",
      "8 [1, 4, 11, 25, 26, 26, 27, 28] [28, 29, 42, 45, 42, 42, 43, 44, 45]\n",
      "8 [1, 4, 17, 52, 42, 45, 42, 42, 43, 44, 45] [66]\n",
      "Total tokens: 15881\n",
      "16 [0] [0, 1, 29, 28, 28, 29]\n",
      "16 [1, 4, 12, 27, 27, 28, 29] [29, 30, 58, 62, 64, 65, 66, 66, 67, 68]\n",
      "16 [1, 4, 24, 51, 57, 61, 63, 64, 65, 66, 66, 67, 68] [68, 69, 101, 121, 126, 126, 127]\n",
      "16 [1, 4, 36, 100, 121, 126, 126, 127] [150]\n",
      "Total tokens: 31864\n",
      "32 [0] [0, 1, 59, 62, 64, 65, 66, 67, 67, 68]\n",
      "32 [1, 4, 24, 51, 57, 61, 63, 65, 66, 67, 67, 68] [68, 69, 136, 152, 151, 151, 152]\n",
      "32 [1, 4, 48, 119, 167, 157, 153, 152, 151, 151, 152] [152, 153, 228, 214, 219, 222, 221, 221, 222]\n",
      "32 [1, 4, 72, 209, 221, 221, 222] [287]\n",
      "Total tokens: 63811\n",
      "64 [0] [0, 1, 118, 166, 157, 153, 152, 151, 151, 152]\n",
      "64 [1, 4, 48, 120, 168, 157, 153, 152, 151, 151, 152] [152, 153, 304, 292, 290, 289, 288, 287, 287, 288]\n",
      "64 [1, 4, 96, 317, 290, 289, 288, 287, 287, 288] [288, 289, 432, 446, 447, 448, 449, 449, 450, 451]\n",
      "64 [1, 4, 145, 451, 450, 450, 451] [606]\n",
      "Total tokens: 1839\n",
      "2 [0] [0, 1, 11, 13, 12, 14, 12, 12, 13]\n",
      "2 [1, 4, 14, 12, 14, 12, 13] [13, 14, 24, 25, 24, 24, 25]\n",
      "2 [1, 4, 29, 24, 25, 24, 24, 25] [25, 26, 35, 38, 39, 39, 40, 41]\n",
      "2 [1, 4, 44, 40, 40, 41] [48]\n",
      "Total tokens: 7836\n",
      "8 [0] [0, 1, 48, 50, 49, 50, 49, 50]\n",
      "8 [1, 4, 63, 40, 57, 40, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50] [50, 51, 87, 68, 80, 68, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n",
      "8 [1, 4, 127, 63, 80, 68, 80, 68, 69, 70, 71, 72, 73, 74, 75, 76] [76, 77, 104, 84, 102, 85, 103, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
      "8 [1, 4, 190, 142, 106, 85, 103, 84, 102, 85, 84, 85, 86, 87, 88, 89, 90, 91, 92] [115]\n",
      "Total tokens: 15860\n",
      "16 [0] [0, 1, 99, 57, 81, 70, 81, 70, 71, 72, 73, 74, 75, 76]\n",
      "16 [1, 4, 128, 57, 81, 70, 81, 70, 71, 72, 73, 74, 75, 76] [76, 77, 140, 122, 115, 116, 116, 117]\n",
      "16 [1, 4, 257, 128, 115, 116, 116, 117] [117, 118, 170, 175, 173, 173, 174]\n",
      "16 [1, 4, 386, 288, 215, 172, 173, 173, 174] [233]\n",
      "Total tokens: 31738\n",
      "32 [0] [0, 1, 198, 112, 115, 116, 116, 117]\n",
      "32 [1, 4, 257, 117, 112, 115, 116, 116, 117] [117, 118, 227, 233, 232, 232, 233]\n",
      "32 [1, 4, 515, 256, 234, 231, 233, 232, 232, 233] [233, 234, 347, 388, 378, 378, 379]\n",
      "32 [1, 4, 773, 578, 432, 371, 380, 378, 378, 379] [502]\n",
      "Total tokens: 63882\n",
      "64 [0] [0, 1, 399, 255, 235, 231, 235, 231, 232, 233, 234]\n",
      "64 [1, 4, 519, 251, 232, 234, 233, 233, 234] [234, 235, 462, 499, 504, 500, 504, 500, 501, 502, 503]\n",
      "64 [1, 4, 1038, 517, 501, 503, 501, 501, 502, 503] [503, 504, 750, 745, 746, 747, 747, 748, 749]\n",
      "64 [1, 4, 1557, 1163, 869, 742, 746, 747, 747, 748, 749] [1021]\n",
      "Total tokens: 1832\n",
      "2 [0] [0, 1, 6, 2, 2, 3]\n",
      "2 [1, 4, 2, 2, 3] [3, 4, 4]\n",
      "2 [1, 4, 4] [4, 5, 7, 8, 9, 10, 11, 11, 12, 13, 14]\n",
      "2 [1, 4, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14] [23]\n",
      "Total tokens: 7892\n",
      "8 [0] [0, 1, 68, 44, 45, 46, 47, 46, 46, 47]\n",
      "8 [1, 4, 49, 47, 46, 47, 46, 47] [47, 48, 95, 72, 86, 72, 72, 73, 74, 75]\n",
      "8 [1, 4, 99, 72, 86, 72, 72, 73, 74, 75] [75, 76, 107, 108, 109, 110, 110, 111]\n",
      "8 [1, 4, 148, 110, 110, 111] [146]\n",
      "Total tokens: 15884\n",
      "16 [0] [0, 1, 136, 75, 71, 87, 73, 78, 70, 89, 72, 86, 73, 72, 73, 74, 75]\n",
      "16 [1, 4, 99, 73, 78, 70, 89, 72, 86, 73, 72, 73, 74, 75] [75, 76, 144, 145, 146, 146, 147]\n",
      "16 [1, 4, 199, 129, 154, 143, 145, 146, 146, 147] [147, 148, 218, 205, 198, 193, 193, 194]\n",
      "16 [1, 4, 299, 224, 206, 198, 193, 193, 194] [281]\n",
      "Total tokens: 31892\n",
      "32 [0] [0, 1, 274, 142, 149, 146, 147, 146, 146, 147]\n",
      "32 [1, 4, 200, 130, 155, 138, 148, 146, 147, 146, 146, 147] [147, 148, 292, 287, 285, 284, 283, 283, 284]\n",
      "32 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284] [284, 285, 426, 420, 422, 422, 423, 424]\n",
      "32 [1, 4, 601, 449, 422, 422, 423, 424] [578]\n",
      "Total tokens: 63799\n",
      "64 [0] [0, 1, 549, 285, 284, 283, 283, 284]\n",
      "64 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284] [284, 285, 568, 573, 575, 576, 576, 577, 578]\n",
      "64 [1, 4, 802, 589, 582, 578, 577, 577, 578] [578, 579, 865, 872, 872, 873, 874]\n",
      "64 [1, 4, 1203, 900, 863, 874, 873, 873, 874] [1195]\n",
      "Total tokens: 1862\n",
      "2 [0] [0, 1, 6, 7, 8, 8, 9]\n",
      "2 [1, 4, 7, 8, 8, 9] [9, 10, 16, 14, 15, 14, 14, 15]\n",
      "2 [1, 4, 14, 15, 14, 14, 15] [15, 16, 22, 23, 24, 25, 26, 26, 27]\n",
      "2 [1, 4, 22, 23, 24, 25, 26, 26, 27] [37]\n",
      "Total tokens: 7857\n",
      "8 [0] [0, 1, 17, 23, 20, 22, 20, 20, 21, 22]\n",
      "8 [1, 4, 34, 24, 21, 22, 20, 22, 20, 21, 22] [22, 23, 41, 49, 54, 56, 57, 58, 58, 59]\n",
      "8 [1, 4, 69, 54, 56, 57, 58, 58, 59] [59, 60, 87, 73, 81, 75, 81, 75, 76, 77, 78]\n",
      "8 [1, 4, 104, 78, 76, 77, 77, 78] [101]\n",
      "Total tokens: 15690\n",
      "16 [0] [0, 1, 35, 47, 52, 55, 57, 57, 58, 59]\n",
      "16 [1, 4, 69, 54, 56, 57, 57, 58, 59] [59, 60, 116, 99, 101, 100, 100, 101]\n",
      "16 [1, 4, 139, 99, 101, 100, 100, 101] [101, 102, 150, 147, 148, 148, 149]\n",
      "16 [1, 4, 209, 156, 149, 148, 148, 149] [185]\n",
      "Total tokens: 31889\n",
      "32 [0] [0, 1, 72, 110, 101, 102, 102, 103]\n",
      "32 [1, 4, 141, 101, 102, 102, 103] [103, 104, 202, 188, 185, 187, 186, 185, 186]\n",
      "32 [1, 4, 283, 235, 208, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186] [186, 187, 279, 354, 391, 413, 425, 425, 426]\n",
      "32 [1, 4, 424, 424, 425, 426] [580]\n",
      "Total tokens: 63756\n",
      "64 [0] [0, 1, 144, 203, 188, 185, 187, 186, 185, 186]\n",
      "64 [1, 4, 283, 236, 209, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186] [186, 187, 372, 535, 581, 580, 580, 581]\n",
      "64 [1, 4, 566, 589, 579, 580, 580, 581] [581, 582, 870, 787, 784, 790, 783, 790, 783, 784, 785, 786, 787]\n",
      "64 [1, 4, 850, 784, 790, 783, 790, 783, 784, 785, 786, 787] [1016]\n",
      "Total tokens: 1894\n",
      "2 [0] [0, 1, 11, 4, 4, 5]\n",
      "2 [1, 4, 4, 5] [5, 6, 7, 6, 6, 7]\n",
      "2 [1, 4, 9, 7, 6, 7, 6, 7] [7, 8, 11, 12, 13, 13, 14]\n",
      "2 [1, 4, 13, 13, 14] [28]\n",
      "Total tokens: 7879\n",
      "8 [0] [0, 1, 46, 33, 30, 30, 31]\n",
      "8 [1, 4, 19, 23, 25, 27, 28, 29, 29, 30, 31] [31, 32, 61, 70, 75, 67, 74, 70, 67, 68, 69, 70, 71, 72]\n",
      "8 [1, 4, 38, 61, 70, 75, 67, 74, 70, 67, 68, 69, 70, 71, 72] [72, 73, 104, 78, 100, 80, 94, 85, 91, 86, 91, 86, 87, 88]\n",
      "8 [1, 4, 58, 103, 79, 96, 82, 95, 83, 92, 85, 91, 86, 91, 86, 87, 88] [106]\n",
      "Total tokens: 15884\n",
      "16 [0] [0, 1, 94, 57, 70, 75, 68, 75, 68, 69, 70, 71, 72]\n",
      "16 [1, 4, 39, 62, 71, 72, 70, 75, 68, 75, 68, 69, 70, 71, 72] [72, 73, 141, 96, 110, 104, 105, 106, 106, 107]\n",
      "16 [1, 4, 78, 134, 98, 111, 104, 105, 106, 106, 107] [107, 108, 156, 145, 142, 144, 143, 144, 143, 144]\n",
      "16 [1, 4, 117, 158, 147, 140, 144, 143, 144, 143, 144] [192]\n",
      "Total tokens: 31715\n",
      "32 [0] [0, 1, 188, 96, 110, 104, 105, 106, 106, 107]\n",
      "32 [1, 4, 77, 133, 97, 111, 104, 105, 106, 106, 107] [107, 108, 207, 181, 188, 192, 191, 191, 192]\n",
      "32 [1, 4, 155, 193, 190, 190, 191, 192] [192, 193, 286, 284, 283, 282, 281, 281, 282]\n",
      "32 [1, 4, 233, 281, 281, 282] [379]\n",
      "Total tokens: 63871\n",
      "64 [0] [0, 1, 380, 189, 193, 192, 193, 192, 193]\n",
      "64 [1, 4, 157, 196, 192, 193, 192, 192, 193] [193, 194, 385, 379, 381, 378, 382, 378, 378, 379, 380]\n",
      "64 [1, 4, 314, 377, 381, 378, 382, 378, 378, 379, 380] [380, 381, 567, 587, 589, 588, 589, 588, 589]\n",
      "64 [1, 4, 471, 563, 590, 588, 589, 588, 588, 589] [844]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "import google.generativeai as genai\n",
    "from config import api_key_google\n",
    "import time\n",
    "\n",
    "tokenizer_name = 'GPT4o'\n",
    "\n",
    "match tokenizer_name:\n",
    "    case \"GPT4\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    case \"GPT4o\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    case \"Claude\":\n",
    "        tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')\n",
    "    case \"Gemini\":\n",
    "        genai.configure(api_key=api_key_google)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\")\n",
    "        tokenizer = model.count_tokens\n",
    "    case \"YaRN\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Yarn-Llama-2-7b-128k\")\n",
    "    case \"Claude_downloaded\":\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/claude-v1-tokenization.json\")\n",
    "    case \"Llama\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    case _:\n",
    "        raise ValueError(\"Unknown tokenizer_name specified. Please provide a valid tokenizer_name.\")\n",
    "\n",
    "ks = [2, 8, 16, 32, 64]\n",
    "\n",
    "for language in  ['English', 'Somali', 'Swahili', 'Indonesian', 'Azeri', 'Vietnamese']:\n",
    "    df_return = pd.DataFrame()\n",
    "    df = pd.read_csv(f\"datasets/Hastacks/mBBC_2024/bbc_{language}.csv\")\n",
    "    for k in ks:\n",
    "        haystack = haystack_builder(df, k, tokenizer)\n",
    "\n",
    "        for percentage in [0, 25, 50, 75]:\n",
    "            parts = split_into_3_parts(haystack, tokenizer, percentage)\n",
    "            needle1, city1, rnd_number1 = needles_builder(language)\n",
    "            needle2, city2, rnd_number2 = needles_builder(language)\n",
    "            data = {\n",
    "                \"context_length\": f\"{k}k\",\n",
    "                \"position\": percentage,\n",
    "                \"text\": parts[0] + ' ' + needle1 + ' ' + parts[1] + ' ' + needle2 + ' ' + parts[2],\n",
    "                \"city1\": city1,\n",
    "                \"label1\": rnd_number1,\n",
    "                \"city2\": city2,\n",
    "                \"label2\": rnd_number2\n",
    "            }\n",
    "            df_return = pd.concat([df_return, pd.DataFrame([data])], ignore_index=True)\n",
    "            df_return.to_csv(f\"datasets/Haystack_Needles/Incongruous/2-Needles/{tokenizer_name}/{language}_needles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d7d7e1057e910",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Three-needles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0c3897cd397b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:12:43.003698Z",
     "start_time": "2024-05-30T16:12:42.995778Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def split_into_4_parts(text, tokenizer, percentage):\n",
    "\n",
    "    # Splitting text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Calculating number of tokens\n",
    "    try:\n",
    "        total_tokens = len(tokenizer.tokenize(text))\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            total_tokens = len(tokenizer.encode(text))\n",
    "        except AttributeError:\n",
    "            total_tokens = tokenizer(text).total_tokens\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Calculating the desired number of tokens for first part\n",
    "    tokens_first_part = int(total_tokens * percentage/100)\n",
    "    tokens_second_part = int(total_tokens * (percentage + 25)/100)\n",
    "\n",
    "    approximated_i1 = [1, 4]\n",
    "\n",
    "    if percentage == 0:\n",
    "        approximated_i1 = [0]\n",
    "    else:\n",
    "        while approximated_i1[-1] != approximated_i1[-2]:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i1[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            approximated_i1.append(int(tokens_first_part / current_length * approximated_i1[-1]))\n",
    "            if approximated_i1[-1] in approximated_i1[:-2]:\n",
    "                approximated_i1.append(min(approximated_i1[-3:-1]))\n",
    "                break\n",
    "        while True:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i1[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i1[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            if current_length < tokens_first_part:\n",
    "                approximated_i1.append(approximated_i1[-1] + 1)\n",
    "            else:\n",
    "                break\n",
    "    if (percentage + 25) != 100:\n",
    "        approximated_i2 = [approximated_i1[-1], approximated_i1[-1] + 1]\n",
    "        while approximated_i2[-1] != approximated_i2[-2]:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i2[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            approximated_i2.append(int(tokens_second_part / current_length * approximated_i2[-1]))\n",
    "            if approximated_i2[-1] in approximated_i2[:-2]:\n",
    "                approximated_i2.append(min(approximated_i2[-3:-1]))\n",
    "                break\n",
    "        while True:\n",
    "            try:\n",
    "                current_length = len(tokenizer.tokenize(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    current_length = len(tokenizer.encode(\" \".join(sentences[:approximated_i2[-1]])))\n",
    "                except AttributeError:\n",
    "                    current_length = tokenizer(\" \".join(sentences[:approximated_i2[-1]])).total_tokens\n",
    "                    time.sleep(1)\n",
    "            if current_length < tokens_second_part:\n",
    "                approximated_i2.append(approximated_i2[-1] + 1)\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        approximated_i2 = [len(sentences)]\n",
    "\n",
    "    # Get the range for selection\n",
    "    start_range = min(approximated_i1[-1], approximated_i2[-1])\n",
    "    end_range = max(approximated_i1[-1], approximated_i2[-1])\n",
    "    \n",
    "    print(start_range, end_range)\n",
    "\n",
    "    # Select 2 numbers randomly from the range\n",
    "    try:\n",
    "        random_numbers = random.sample(range(start_range, end_range + 1), 2)\n",
    "    except ValueError:\n",
    "        random_numbers = [start_range, end_range]\n",
    "    random_numbers.sort()\n",
    "\n",
    "    parts = [' '.join(sentences[:approximated_i1[-1]]), ' '.join(sentences[approximated_i1[-1]:random_numbers[0]]), ' '.join(sentences[random_numbers[0]:random_numbers[1]]), ' '.join(sentences[random_numbers[1]:])]\n",
    "    print(k, approximated_i1, approximated_i2)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817b763772ef3632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:13:55.657645Z",
     "start_time": "2024-05-30T16:13:41.465467Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1839\n",
      "0 8\n",
      "2 [0] [0, 1, 5, 5, 6, 7, 8]\n",
      "8 18\n",
      "2 [1, 4, 5, 5, 6, 7, 8] [8, 9, 15, 15, 16, 17, 18]\n",
      "18 23\n",
      "2 [1, 4, 10, 15, 15, 16, 17, 18] [18, 19, 22, 22, 23]\n",
      "23 33\n",
      "2 [1, 4, 15, 23, 22, 22, 23] [33]\n",
      "Total tokens: 7887\n",
      "0 36\n",
      "8 [0] [0, 1, 22, 31, 34, 34, 35, 36]\n",
      "36 73\n",
      "8 [1, 4, 21, 31, 34, 34, 35, 36] [36, 37, 72, 72, 73]\n",
      "73 123\n",
      "8 [1, 4, 43, 73, 72, 72, 73] [73, 74, 109, 123, 122, 123, 122, 123]\n",
      "123 166\n",
      "8 [1, 4, 65, 99, 124, 123, 122, 123, 122, 123] [166]\n",
      "Total tokens: 15891\n",
      "0 74\n",
      "16 [0] [0, 1, 45, 70, 70, 71, 72, 73, 74]\n",
      "74 167\n",
      "16 [1, 4, 43, 74, 73, 73, 74] [74, 75, 147, 159, 161, 162, 163, 164, 164, 165, 166, 167]\n",
      "167 257\n",
      "16 [1, 4, 87, 156, 161, 162, 163, 164, 164, 165, 166, 167] [167, 168, 250, 253, 254, 255, 255, 256, 257]\n",
      "257 360\n",
      "16 [1, 4, 130, 239, 247, 252, 254, 255, 255, 256, 257] [360]\n",
      "Total tokens: 31889\n",
      "0 168\n",
      "32 [0] [0, 1, 91, 158, 162, 163, 164, 165, 166, 166, 167, 168]\n",
      "168 360\n",
      "32 [1, 4, 87, 156, 161, 163, 164, 165, 166, 166, 167, 168] [168, 169, 333, 367, 360, 358, 362, 357, 362, 357, 358, 359, 360]\n",
      "360 510\n",
      "32 [1, 4, 175, 337, 367, 360, 358, 362, 357, 362, 357, 358, 359, 360] [360, 361, 537, 513, 508, 511, 508, 508, 509, 510]\n",
      "510 744\n",
      "32 [1, 4, 262, 522, 509, 509, 510] [744]\n",
      "Total tokens: 63852\n",
      "0 360\n",
      "64 [0] [0, 1, 183, 342, 360, 359, 363, 358, 363, 358, 359, 360]\n",
      "360 740\n",
      "64 [1, 4, 175, 338, 366, 360, 359, 363, 358, 363, 358, 359, 360] [360, 361, 717, 730, 740, 739, 739, 740]\n",
      "740 1061\n",
      "64 [1, 4, 350, 720, 733, 737, 738, 739, 739, 740] [740, 741, 1110, 1054, 1058, 1058, 1059, 1060, 1061]\n",
      "1061 1398\n",
      "64 [1, 4, 526, 1022, 1062, 1061, 1060, 1060, 1061] [1398]\n",
      "Total tokens: 1871\n",
      "0 2\n",
      "2 [0] [0, 1, 3, 1, 1, 2]\n",
      "2 3\n",
      "2 [1, 4, 1, 1, 2] [2, 3, 2, 2, 3]\n",
      "3 6\n",
      "2 [1, 4, 2, 3, 2, 2, 3] [3, 4, 4, 5, 6]\n",
      "6 16\n",
      "2 [1, 4, 4, 5, 6] [16]\n",
      "Total tokens: 7840\n",
      "0 18\n",
      "8 [0] [0, 1, 14, 15, 16, 16, 17, 18]\n",
      "18 28\n",
      "8 [1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 17, 18] [18, 19, 36, 33, 30, 28, 27, 27, 28]\n",
      "28 45\n",
      "8 [1, 4, 11, 25, 26, 26, 27, 28] [28, 29, 42, 45, 42, 42, 43, 44, 45]\n",
      "45 66\n",
      "8 [1, 4, 17, 52, 42, 45, 42, 42, 43, 44, 45] [66]\n",
      "Total tokens: 15881\n",
      "0 29\n",
      "16 [0] [0, 1, 29, 28, 28, 29]\n",
      "29 68\n",
      "16 [1, 4, 12, 27, 27, 28, 29] [29, 30, 58, 62, 64, 65, 66, 66, 67, 68]\n",
      "68 127\n",
      "16 [1, 4, 24, 51, 57, 61, 63, 64, 65, 66, 66, 67, 68] [68, 69, 101, 121, 126, 126, 127]\n",
      "127 150\n",
      "16 [1, 4, 36, 100, 121, 126, 126, 127] [150]\n",
      "Total tokens: 31864\n",
      "0 68\n",
      "32 [0] [0, 1, 59, 62, 64, 65, 66, 67, 67, 68]\n",
      "68 152\n",
      "32 [1, 4, 24, 51, 57, 61, 63, 65, 66, 67, 67, 68] [68, 69, 136, 152, 151, 151, 152]\n",
      "152 222\n",
      "32 [1, 4, 48, 119, 167, 157, 153, 152, 151, 151, 152] [152, 153, 228, 214, 219, 222, 221, 221, 222]\n",
      "222 287\n",
      "32 [1, 4, 72, 209, 221, 221, 222] [287]\n",
      "Total tokens: 63811\n",
      "0 152\n",
      "64 [0] [0, 1, 118, 166, 157, 153, 152, 151, 151, 152]\n",
      "152 288\n",
      "64 [1, 4, 48, 120, 168, 157, 153, 152, 151, 151, 152] [152, 153, 304, 292, 290, 289, 288, 287, 287, 288]\n",
      "288 451\n",
      "64 [1, 4, 96, 317, 290, 289, 288, 287, 287, 288] [288, 289, 432, 446, 447, 448, 449, 449, 450, 451]\n",
      "451 606\n",
      "64 [1, 4, 145, 451, 450, 450, 451] [606]\n",
      "Total tokens: 1839\n",
      "0 13\n",
      "2 [0] [0, 1, 11, 13, 12, 14, 12, 12, 13]\n",
      "13 25\n",
      "2 [1, 4, 14, 12, 14, 12, 13] [13, 14, 24, 25, 24, 24, 25]\n",
      "25 41\n",
      "2 [1, 4, 29, 24, 25, 24, 24, 25] [25, 26, 35, 38, 39, 39, 40, 41]\n",
      "41 48\n",
      "2 [1, 4, 44, 40, 40, 41] [48]\n",
      "Total tokens: 7836\n",
      "0 50\n",
      "8 [0] [0, 1, 48, 50, 49, 50, 49, 50]\n",
      "50 76\n",
      "8 [1, 4, 63, 40, 57, 40, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50] [50, 51, 87, 68, 80, 68, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n",
      "76 92\n",
      "8 [1, 4, 127, 63, 80, 68, 80, 68, 69, 70, 71, 72, 73, 74, 75, 76] [76, 77, 104, 84, 102, 85, 103, 84, 85, 86, 87, 88, 89, 90, 91, 92]\n",
      "92 115\n",
      "8 [1, 4, 190, 142, 106, 85, 103, 84, 102, 85, 84, 85, 86, 87, 88, 89, 90, 91, 92] [115]\n",
      "Total tokens: 15860\n",
      "0 76\n",
      "16 [0] [0, 1, 99, 57, 81, 70, 81, 70, 71, 72, 73, 74, 75, 76]\n",
      "76 117\n",
      "16 [1, 4, 128, 57, 81, 70, 81, 70, 71, 72, 73, 74, 75, 76] [76, 77, 140, 122, 115, 116, 116, 117]\n",
      "117 174\n",
      "16 [1, 4, 257, 128, 115, 116, 116, 117] [117, 118, 170, 175, 173, 173, 174]\n",
      "174 233\n",
      "16 [1, 4, 386, 288, 215, 172, 173, 173, 174] [233]\n",
      "Total tokens: 31738\n",
      "0 117\n",
      "32 [0] [0, 1, 198, 112, 115, 116, 116, 117]\n",
      "117 233\n",
      "32 [1, 4, 257, 117, 112, 115, 116, 116, 117] [117, 118, 227, 233, 232, 232, 233]\n",
      "233 379\n",
      "32 [1, 4, 515, 256, 234, 231, 233, 232, 232, 233] [233, 234, 347, 388, 378, 378, 379]\n",
      "379 502\n",
      "32 [1, 4, 773, 578, 432, 371, 380, 378, 378, 379] [502]\n",
      "Total tokens: 63882\n",
      "0 234\n",
      "64 [0] [0, 1, 399, 255, 235, 231, 235, 231, 232, 233, 234]\n",
      "234 503\n",
      "64 [1, 4, 519, 251, 232, 234, 233, 233, 234] [234, 235, 462, 499, 504, 500, 504, 500, 501, 502, 503]\n",
      "503 749\n",
      "64 [1, 4, 1038, 517, 501, 503, 501, 501, 502, 503] [503, 504, 750, 745, 746, 747, 747, 748, 749]\n",
      "749 1021\n",
      "64 [1, 4, 1557, 1163, 869, 742, 746, 747, 747, 748, 749] [1021]\n",
      "Total tokens: 1832\n",
      "0 3\n",
      "2 [0] [0, 1, 6, 2, 2, 3]\n",
      "3 4\n",
      "2 [1, 4, 2, 2, 3] [3, 4, 4]\n",
      "4 14\n",
      "2 [1, 4, 4] [4, 5, 7, 8, 9, 10, 11, 11, 12, 13, 14]\n",
      "14 23\n",
      "2 [1, 4, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14] [23]\n",
      "Total tokens: 7892\n",
      "0 47\n",
      "8 [0] [0, 1, 68, 44, 45, 46, 47, 46, 46, 47]\n",
      "47 75\n",
      "8 [1, 4, 49, 47, 46, 47, 46, 47] [47, 48, 95, 72, 86, 72, 72, 73, 74, 75]\n",
      "75 111\n",
      "8 [1, 4, 99, 72, 86, 72, 72, 73, 74, 75] [75, 76, 107, 108, 109, 110, 110, 111]\n",
      "111 146\n",
      "8 [1, 4, 148, 110, 110, 111] [146]\n",
      "Total tokens: 15884\n",
      "0 75\n",
      "16 [0] [0, 1, 136, 75, 71, 87, 73, 78, 70, 89, 72, 86, 73, 72, 73, 74, 75]\n",
      "75 147\n",
      "16 [1, 4, 99, 73, 78, 70, 89, 72, 86, 73, 72, 73, 74, 75] [75, 76, 144, 145, 146, 146, 147]\n",
      "147 194\n",
      "16 [1, 4, 199, 129, 154, 143, 145, 146, 146, 147] [147, 148, 218, 205, 198, 193, 193, 194]\n",
      "194 281\n",
      "16 [1, 4, 299, 224, 206, 198, 193, 193, 194] [281]\n",
      "Total tokens: 31892\n",
      "0 147\n",
      "32 [0] [0, 1, 274, 142, 149, 146, 147, 146, 146, 147]\n",
      "147 284\n",
      "32 [1, 4, 200, 130, 155, 138, 148, 146, 147, 146, 146, 147] [147, 148, 292, 287, 285, 284, 283, 283, 284]\n",
      "284 424\n",
      "32 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284] [284, 285, 426, 420, 422, 422, 423, 424]\n",
      "424 578\n",
      "32 [1, 4, 601, 449, 422, 422, 423, 424] [578]\n",
      "Total tokens: 63799\n",
      "0 284\n",
      "64 [0] [0, 1, 549, 285, 284, 283, 283, 284]\n",
      "284 578\n",
      "64 [1, 4, 401, 290, 287, 285, 284, 283, 283, 284] [284, 285, 568, 573, 575, 576, 576, 577, 578]\n",
      "578 874\n",
      "64 [1, 4, 802, 589, 582, 578, 577, 577, 578] [578, 579, 865, 872, 872, 873, 874]\n",
      "874 1195\n",
      "64 [1, 4, 1203, 900, 863, 874, 873, 873, 874] [1195]\n",
      "Total tokens: 1862\n",
      "0 9\n",
      "2 [0] [0, 1, 6, 7, 8, 8, 9]\n",
      "9 15\n",
      "2 [1, 4, 7, 8, 8, 9] [9, 10, 16, 14, 15, 14, 14, 15]\n",
      "15 27\n",
      "2 [1, 4, 14, 15, 14, 14, 15] [15, 16, 22, 23, 24, 25, 26, 26, 27]\n",
      "27 37\n",
      "2 [1, 4, 22, 23, 24, 25, 26, 26, 27] [37]\n",
      "Total tokens: 7857\n",
      "0 22\n",
      "8 [0] [0, 1, 17, 23, 20, 22, 20, 20, 21, 22]\n",
      "22 59\n",
      "8 [1, 4, 34, 24, 21, 22, 20, 22, 20, 21, 22] [22, 23, 41, 49, 54, 56, 57, 58, 58, 59]\n",
      "59 78\n",
      "8 [1, 4, 69, 54, 56, 57, 58, 58, 59] [59, 60, 87, 73, 81, 75, 81, 75, 76, 77, 78]\n",
      "78 101\n",
      "8 [1, 4, 104, 78, 76, 77, 77, 78] [101]\n",
      "Total tokens: 15690\n",
      "0 59\n",
      "16 [0] [0, 1, 35, 47, 52, 55, 57, 57, 58, 59]\n",
      "59 101\n",
      "16 [1, 4, 69, 54, 56, 57, 57, 58, 59] [59, 60, 116, 99, 101, 100, 100, 101]\n",
      "101 149\n",
      "16 [1, 4, 139, 99, 101, 100, 100, 101] [101, 102, 150, 147, 148, 148, 149]\n",
      "149 185\n",
      "16 [1, 4, 209, 156, 149, 148, 148, 149] [185]\n",
      "Total tokens: 31889\n",
      "0 103\n",
      "32 [0] [0, 1, 72, 110, 101, 102, 102, 103]\n",
      "103 186\n",
      "32 [1, 4, 141, 101, 102, 102, 103] [103, 104, 202, 188, 185, 187, 186, 185, 186]\n",
      "186 426\n",
      "32 [1, 4, 283, 235, 208, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186] [186, 187, 279, 354, 391, 413, 425, 425, 426]\n",
      "426 580\n",
      "32 [1, 4, 424, 424, 425, 426] [580]\n",
      "Total tokens: 63756\n",
      "0 186\n",
      "64 [0] [0, 1, 144, 203, 188, 185, 187, 186, 185, 186]\n",
      "186 581\n",
      "64 [1, 4, 283, 236, 209, 191, 180, 189, 184, 188, 185, 187, 186, 185, 186] [186, 187, 372, 535, 581, 580, 580, 581]\n",
      "581 787\n",
      "64 [1, 4, 566, 589, 579, 580, 580, 581] [581, 582, 870, 787, 784, 790, 783, 790, 783, 784, 785, 786, 787]\n",
      "787 1016\n",
      "64 [1, 4, 850, 784, 790, 783, 790, 783, 784, 785, 786, 787] [1016]\n",
      "Total tokens: 1894\n",
      "0 5\n",
      "2 [0] [0, 1, 11, 4, 4, 5]\n",
      "5 7\n",
      "2 [1, 4, 4, 5] [5, 6, 7, 6, 6, 7]\n",
      "7 14\n",
      "2 [1, 4, 9, 7, 6, 7, 6, 7] [7, 8, 11, 12, 13, 13, 14]\n",
      "14 28\n",
      "2 [1, 4, 13, 13, 14] [28]\n",
      "Total tokens: 7879\n",
      "0 31\n",
      "8 [0] [0, 1, 46, 33, 30, 30, 31]\n",
      "31 72\n",
      "8 [1, 4, 19, 23, 25, 27, 28, 29, 29, 30, 31] [31, 32, 61, 70, 75, 67, 74, 70, 67, 68, 69, 70, 71, 72]\n",
      "72 88\n",
      "8 [1, 4, 38, 61, 70, 75, 67, 74, 70, 67, 68, 69, 70, 71, 72] [72, 73, 104, 78, 100, 80, 94, 85, 91, 86, 91, 86, 87, 88]\n",
      "88 106\n",
      "8 [1, 4, 58, 103, 79, 96, 82, 95, 83, 92, 85, 91, 86, 91, 86, 87, 88] [106]\n",
      "Total tokens: 15884\n",
      "0 72\n",
      "16 [0] [0, 1, 94, 57, 70, 75, 68, 75, 68, 69, 70, 71, 72]\n",
      "72 107\n",
      "16 [1, 4, 39, 62, 71, 72, 70, 75, 68, 75, 68, 69, 70, 71, 72] [72, 73, 141, 96, 110, 104, 105, 106, 106, 107]\n",
      "107 144\n",
      "16 [1, 4, 78, 134, 98, 111, 104, 105, 106, 106, 107] [107, 108, 156, 145, 142, 144, 143, 144, 143, 144]\n",
      "144 192\n",
      "16 [1, 4, 117, 158, 147, 140, 144, 143, 144, 143, 144] [192]\n",
      "Total tokens: 31715\n",
      "0 107\n",
      "32 [0] [0, 1, 188, 96, 110, 104, 105, 106, 106, 107]\n",
      "107 192\n",
      "32 [1, 4, 77, 133, 97, 111, 104, 105, 106, 106, 107] [107, 108, 207, 181, 188, 192, 191, 191, 192]\n",
      "192 282\n",
      "32 [1, 4, 155, 193, 190, 190, 191, 192] [192, 193, 286, 284, 283, 282, 281, 281, 282]\n",
      "282 379\n",
      "32 [1, 4, 233, 281, 281, 282] [379]\n",
      "Total tokens: 63871\n",
      "0 193\n",
      "64 [0] [0, 1, 380, 189, 193, 192, 193, 192, 193]\n",
      "193 380\n",
      "64 [1, 4, 157, 196, 192, 193, 192, 192, 193] [193, 194, 385, 379, 381, 378, 382, 378, 378, 379, 380]\n",
      "380 589\n",
      "64 [1, 4, 314, 377, 381, 378, 382, 378, 378, 379, 380] [380, 381, 567, 587, 589, 588, 589, 588, 589]\n",
      "589 844\n",
      "64 [1, 4, 471, 563, 590, 588, 589, 588, 588, 589] [844]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "import google.generativeai as genai\n",
    "from config import api_key_google\n",
    "import time\n",
    "\n",
    "tokenizer_name = 'GPT4o'\n",
    "\n",
    "match tokenizer_name:\n",
    "    case \"GPT4\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    case \"GPT4o\":\n",
    "        tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    case \"Claude\":\n",
    "        tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')\n",
    "    case \"Gemini\":\n",
    "        genai.configure(api_key=api_key_google)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\")\n",
    "        tokenizer = model.count_tokens\n",
    "    case \"YaRN\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Yarn-Llama-2-7b-128k\")\n",
    "    case \"Claude_downloaded\":\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/claude-v1-tokenization.json\")\n",
    "    case \"Llama\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    case _:\n",
    "        raise ValueError(\"Unknown tokenizer_name specified. Please provide a valid tokenizer_name.\")\n",
    "\n",
    "ks = [2, 8, 16, 32, 64]\n",
    "\n",
    "for language in ['English', 'Somali', 'Swahili', 'Indonesian', 'Azeri', 'Vietnamese']:\n",
    "    df_return = pd.DataFrame()\n",
    "    df = pd.read_csv(f\"datasets/Hastacks/mBBC_2024/bbc_{language}.csv\")\n",
    "    for k in ks:\n",
    "        haystack = haystack_builder(df, k, tokenizer)\n",
    "\n",
    "        for percentage in [0, 25, 50, 75]:\n",
    "            parts = split_into_4_parts(haystack, tokenizer, percentage)\n",
    "            city1, city2, city3 = \"\", \"\", \"\"\n",
    "            while city1 == city2 or city1 == city3 or city2 == city3:\n",
    "                needle1, city1, rnd_number1 = needles_builder(language)\n",
    "                needle2, city2, rnd_number2 = needles_builder(language)\n",
    "                needle3, city3, rnd_number3 = needles_builder(language)\n",
    "            data = {\n",
    "                \"context_length\": f\"{k}k\",\n",
    "                \"position\": percentage,\n",
    "                \"text\": parts[0] + ' ' + needle1 + ' ' + parts[1] + ' ' + needle2 + ' ' + parts[2] + ' ' + needle3 + ' ' + parts[3],\n",
    "                \"city1\": city1,\n",
    "                \"label1\": rnd_number1,\n",
    "                \"city2\": city2,\n",
    "                \"label2\": rnd_number2,\n",
    "                \"city3\": city3,\n",
    "                \"label3\": rnd_number3\n",
    "            }\n",
    "            df_return = pd.concat([df_return, pd.DataFrame([data])], ignore_index=True)\n",
    "            df_return.to_csv(f\"datasets/Haystack_Needles/Incongruous/3-Needles/{tokenizer_name}/{language}_needles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfcee152fec88ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate fertility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09eca7dd1e59563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:19:29.019949Z",
     "start_time": "2024-05-30T16:19:29.016459Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+-----------+\n",
      "|            |   GPT-4 |   GPT-4o |   Llama-3 |\n",
      "+============+=========+==========+===========+\n",
      "| English    | 1.13002 |  1.113   |   1.12959 |\n",
      "+------------+---------+----------+-----------+\n",
      "| Somali     | 2.37535 |  1.78619 |   2.35605 |\n",
      "+------------+---------+----------+-----------+\n",
      "| Swahili    | 2.23791 |  1.67568 |   2.21385 |\n",
      "+------------+---------+----------+-----------+\n",
      "| Indonesian | 1.92439 |  1.55291 |   1.91363 |\n",
      "+------------+---------+----------+-----------+\n",
      "| Azeri      | 3.37034 |  2.17708 |   3.10339 |\n",
      "+------------+---------+----------+-----------+\n",
      "| Vietnamese | 2.08932 |  1.2865  |   1.27364 |\n",
      "+------------+---------+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tabulate import tabulate\n",
    "from transformers import GPT2TokenizerFast, AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Iterate over languages\n",
    "for language in ['English', 'Somali', 'Swahili', 'Indonesian', 'Azeri', 'Vietnamese']:\n",
    "    # Initialize a list to store results for this language\n",
    "    language_results = []\n",
    "\n",
    "    # Iterate over tokenizer names\n",
    "    for tokenizer_name in ['GPT4', 'GPT4o', 'Llama']:\n",
    "        match tokenizer_name:\n",
    "            case \"GPT4\":\n",
    "                tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "            case \"GPT4o\":\n",
    "                tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "            case \"Claude\":\n",
    "                tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/claude-tokenizer')\n",
    "            case \"Gemini\":\n",
    "                genai.configure(api_key=api_key_google)\n",
    "                model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\")\n",
    "                tokenizer = model.count_tokens\n",
    "            case \"YaRN\":\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Yarn-Llama-2-7b-128k\")\n",
    "            case \"Claude_downloaded\":\n",
    "                tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/claude-v1-tokenization.json\")\n",
    "            case \"Llama\":\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown tokenizer_name specified. Please provide a valid tokenizer_name.\")\n",
    "\n",
    "        # Process text and calculate fertility score\n",
    "        df = pd.read_csv(f\"datasets/Hastacks/mBBC_2024/bbc_{language}.csv\")\n",
    "        text = \" \".join(df['body'][:50])\n",
    "        try:\n",
    "            total_tokens = len(tokenizer.tokenize(text))\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                total_tokens = len(tokenizer.encode(text))\n",
    "            except AttributeError:\n",
    "                total_tokens = tokenizer(text).total_tokens\n",
    "                time.sleep(1)\n",
    "        total_words = len(word_tokenize(text))\n",
    "        fertility_score = total_tokens / total_words\n",
    "\n",
    "        # Append fertility score to language_results list\n",
    "        language_results.append(fertility_score)\n",
    "\n",
    "    # Add language_results to the results dictionary with the language as key\n",
    "    results[language] = language_results\n",
    "\n",
    "# Print the results in a table format\n",
    "tokenizers = ['GPT-4', 'GPT-4o', 'Llama-3']\n",
    "print(tabulate(results.values(), headers=tokenizers, showindex=results.keys(), tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bc0c9908d7f1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Find Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1786085363e9c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:52:06.182952Z",
     "start_time": "2024-03-28T22:52:06.054174Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "for language in ['English']:\n",
    "    df = pd.read_csv(f\"datasets/Hastacks/mBBC_2024/bbc_{language}.csv\")\n",
    "    ## filter dataframe which the url has 'articles'\n",
    "    df = df[df['url'].str.contains('articles')]\n",
    "    \n",
    "    # sort df by the length of the body\n",
    "    df['body_length'] = df['body'].apply(lambda x: len(x.split()))\n",
    "    df = df.sort_values(by='body_length', ascending=False, ignore_index=True)[:40]\n",
    "    \n",
    "    #shuffle the dataframe\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    dataset = pd.DataFrame()\n",
    "    for length in [10, 20, 30, 40]:\n",
    "        concatenated_bodies = [f\"Article {index+1}: {body}\" for index, body in df['body'][:length].items()]\n",
    "        concatenated_bodies = \"\\n\".join(concatenated_bodies)\n",
    "        for depth in [0, 25, 50, 75, 100]:\n",
    "            # calculate the index of the article by ceiling division\n",
    "            depth_index = 0 if depth ==0 else math.ceil(length * depth / 100)-1\n",
    "            dictionary = {\n",
    "                \"context_length\": f\"{length}\",\n",
    "                \"position\": f\"%{depth}\",\n",
    "                \"text\": concatenated_bodies,\n",
    "                \"title\": df['title'][depth_index],\n",
    "                \"label\": f\"Article {depth_index+1}\"\n",
    "            }\n",
    "            dataset = pd.concat([dataset, pd.DataFrame([dictionary])], ignore_index=True)\n",
    "    # dataset.to_csv(f\"datasets/Find_article/{language}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b7da0b13ffab236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T05:38:44.451326Z",
     "start_time": "2024-03-30T05:38:22.364763Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numbers in English haystack: 711\n",
      "Number of geopolitical entities in English haystack: 572\n",
      "Number of numbers in Somali haystack: 148\n",
      "Number of geopolitical entities in Somali haystack: 1054\n",
      "Number of numbers in Swahili haystack: 419\n",
      "Number of geopolitical entities in Swahili haystack: 1347\n",
      "Number of numbers in Indonesian haystack: 664\n",
      "Number of geopolitical entities in Indonesian haystack: 1786\n",
      "Number of numbers in Vietnamese haystack: 636\n",
      "Number of geopolitical entities in Vietnamese haystack: 790\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "def count_cities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    city_count = sum(1 for ent in doc.ents if ent.label_ == 'GPE')  # Count entities labeled as geopolitical entities (cities)\n",
    "    return city_count\n",
    "\n",
    "def count_numbers(text):\n",
    "    numbers = re.findall(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b', text.replace(\",\", \"\"))  # Remove commas temporarily\n",
    "    return len(numbers)\n",
    "\n",
    "for language in ['English', 'Somali', 'Swahili', 'Indonesian', 'Vietnamese']:\n",
    "    df = pd.read_csv(f\"datasets/Haystack_Needles/Incongruous/3-Needles/Gemini/{language}_needles.csv\")\n",
    "    text = list(df['text'])[-1]\n",
    "    print(f\"Number of numbers in {language} haystack: {count_numbers(text) - 3}\")\n",
    "    print(f\"Number of geopolitical entities in {language} haystack: {count_cities(text) - 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8620c6d394844",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
